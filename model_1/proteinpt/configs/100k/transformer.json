{
  "train_data_path": "data/100k/train_coords.txt",
  "validation_data_path": "data/100k/valid_coords.txt",
  "vocabulary": {
    "directory_path": "proteinpt/vocab",
    "extend": false
  },

  "dataset_reader": {
    "type": "protein_reader",
    "lazy": true
  },

  "model": {
    "type": "protein_model",
    "target": "dcalpha",
    "aa_embedder": {
      "aa": {
        "type": "embedding",
        "embedding_dim": 64,
        "trainable": true,
        "vocab_namespace": "aa"
      }
    },
    "ss_embedder": {
      "ss": {
        "type": "embedding",
        "embedding_dim": 64,
        "trainable": true,
        "vocab_namespace": "ss"
      }
    },
    "encoder": {
      "type": "stacked_self_attention",
      "input_dim": 64,
      "hidden_dim": 64,
      "projection_dim": 64,
      "feedforward_hidden_dim": 256,
      "num_layers": 2,
      "num_attention_heads": 4,
      "use_positional_encoding": true
    },
    "feedforward": {
      "input_dim": 64,
      "num_layers": 2,
      "hidden_dims": 64,
      "activations": "relu",
      "dropout": 0.2
    },
    "use_ss": false,
    "use_positional_encoding": false,
    "input_dropout": 0.2,
    "layer_norm": true,
    "residual": true
  },

  "iterator": {
    "type": "basic",
    "batch_size": 32,
    "max_instances_in_memory": 2000
  },

  "trainer": {
    "num_serialized_models_to_keep": 1,
    "num_epochs": 9999999,
    "patience": 20,
    "cuda_device": 0,
    "optimizer": {
      "type": "adam",
      "lr": 0.001
    },
    "learning_rate_scheduler": {
      "type": "noam",
      "model_size": 64,
      "warmup_steps": 8000,
      "factor": 1.0
    },
    "should_log_learning_rate": true
  }
}